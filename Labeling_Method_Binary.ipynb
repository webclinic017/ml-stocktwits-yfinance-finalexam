{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Labeling Method Binary.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hMJ4bwWr2Nfg"
      },
      "outputs": [],
      "source": [
        "!pip install contractions\n",
        "!pip install emoji\n",
        "!pip install ekphrasis\n",
        "!pip install -U -q PyDrive\n",
        "!pip install yfinance --upgrade --no-cache-dir\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "import emoji\n",
        "import nltk\n",
        "import contractions\n",
        "import torch\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.tokenize import TweetTokenizer \n",
        "from nltk.corpus import stopwords as sw\n",
        "from nltk.tokenize import word_tokenize \n",
        "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
        "import requests \n",
        "from pprint import pprint\n",
        "import numpy as np\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from datetime import datetime\n",
        "from tensorflow import keras\n",
        "\n",
        "import os\n",
        "import pprint\n",
        "import json\n",
        "import random\n",
        "import string\n",
        "import sys\n",
        "from ekphrasis.classes.segmenter import Segmenter\n",
        "import itertools\n",
        "\n",
        "\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "import pandas as pd\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.ticker import FormatStrFormatter, StrMethodFormatter\n",
        "\n",
        "import datetime\n",
        "from datetime import datetime, timedelta, date\n",
        "\n",
        "import yfinance as yf\n",
        "from datetime import datetime, timedelta, date\n",
        "\n",
        "#mount's to Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "#connect's to Google Cloud SDK\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "#Definition get_compData we will call back to later to gather the data to a dataframe\n",
        "def get_compData(comp):\n",
        "  #If what we call back equals AAPL\n",
        "  if comp =='AAPL':\n",
        "    # AAPL: https://drive.google.com/file/d/1diLooyj8DtyyxwiihpbTJdfY4D98irLE/view?usp=sharing\n",
        "    \n",
        "    #ID of the file that is unique and from google drive\n",
        "    id = '1diLooyj8DtyyxwiihpbTJdfY4D98irLE'\n",
        "    \n",
        "    #download1 is the command create file with the id from over as id.\n",
        "    downloaded1 = drive.CreateFile({'id':id}) \n",
        "\n",
        "    #Gets the content from the file with the id and download it.\n",
        "    downloaded1.GetContentFile('stocktwits_AAPL.csv')\n",
        "\n",
        "    #Places the content from over in a new dataframe\n",
        "    df_st1 = pd.read_csv('stocktwits_AAPL.csv')\n",
        "\n",
        "    #Returns the df_st1 from where we ran our get_compData(comp) code.\n",
        "    return df_st1\n",
        "  elif comp=='ADBE':\n",
        "    # ADBE: https://drive.google.com/file/d/1SMyRg8aUnnQTbukVadBo814qdS6xDnP1/view?usp=sharing\n",
        "    id = '1SMyRg8aUnnQTbukVadBo814qdS6xDnP1'\n",
        "    downloaded2 = drive.CreateFile({'id':id}) \n",
        "    downloaded2.GetContentFile('stocktwits_ADBE.csv')\n",
        "    df_st2 = pd.read_csv('stocktwits_ADBE.csv')\n",
        "    return df_st2\n",
        "  elif comp=='AMZN':\n",
        "    # AMZN: https://drive.google.com/file/d/1ffmFOrlcaTCCByKMq7E1LCXXK6daMkB_/view?usp=sharing\n",
        "    id = '1ffmFOrlcaTCCByKMq7E1LCXXK6daMkB_'\n",
        "    downloaded3 = drive.CreateFile({'id':id}) \n",
        "    downloaded3.GetContentFile('stocktwits_AMZN.csv')\n",
        "    df_st3 = pd.read_csv('stocktwits_AMZN.csv')\n",
        "    return df_st3\n",
        "  elif comp=='BAC':\n",
        "    # BAC: https://drive.google.com/file/d/1HeaKJtlSz2xiLT3sx5FLdYmzCkVDGIjW/view?usp=sharing\n",
        "    id = '1HeaKJtlSz2xiLT3sx5FLdYmzCkVDGIjW'\n",
        "    downloaded4 = drive.CreateFile({'id':id}) \n",
        "    downloaded4.GetContentFile('stocktwits_BAC.csv')\n",
        "    df_st4 = pd.read_csv('stocktwits_BAC.csv')\n",
        "    return df_st4\n",
        "  elif comp=='BRK.A':\n",
        "    #BRK_A: https://drive.google.com/file/d/1HeQhAU20YyT1tRCD-yN83vrUGY7jDDSt/view?usp=sharing\n",
        "    id = '1HeQhAU20YyT1tRCD-yN83vrUGY7jDDSt'\n",
        "    downloaded5 = drive.CreateFile({'id':id}) \n",
        "    downloaded5.GetContentFile('stocktwits_BRK_A.csv')\n",
        "    df_st5 = pd.read_csv('stocktwits_BRK_A.csv')\n",
        "    return df_st5\n",
        "  elif comp=='BRK.B':\n",
        "    #BRK_B: https://drive.google.com/file/d/1FRrwrIVJVyFqg025SWasKW84qOJMhe84/view?usp=sharing\n",
        "    id = '1FRrwrIVJVyFqg025SWasKW84qOJMhe84'\n",
        "    downloaded6 = drive.CreateFile({'id':id}) \n",
        "    downloaded6.GetContentFile('stocktwits_BRK_B.csv')\n",
        "    df_st6 = pd.read_csv('stocktwits_BRK_B.csv')\n",
        "    return df_st6\n",
        "  elif comp=='DIA':\n",
        "    #DIA: https://drive.google.com/file/d/1riZ8IdkupLre9NQ7McBO21wDFVX_NPvg/view?usp=sharing\n",
        "    id = '1riZ8IdkupLre9NQ7McBO21wDFVX_NPvg'\n",
        "    downloaded7 = drive.CreateFile({'id':id}) \n",
        "    downloaded7.GetContentFile('stocktwits_DIA.csv')\n",
        "    df_st7 = pd.read_csv('stocktwits_DIA.csv')\n",
        "    return df_st7\n",
        "  elif comp=='DIS':\n",
        "    #DIS: https://drive.google.com/file/d/1y5IT3gA_yIJnFLTsxy1GHdIY4SHwj84Y/view?usp=sharing\n",
        "    id = '1y5IT3gA_yIJnFLTsxy1GHdIY4SHwj84Y'\n",
        "    downloaded8 = drive.CreateFile({'id':id}) \n",
        "    downloaded8.GetContentFile('stocktwits_DIS.csv')\n",
        "    df_st8 = pd.read_csv('stocktwits_DIS.csv')\n",
        "    return df_st8\n",
        "  elif comp=='FB':\n",
        "    #FB: https://drive.google.com/file/d/1x1FugJhUQx9xKWS9LYnio1MU8oi5iuTc/view?usp=sharing\n",
        "    id = '1x1FugJhUQx9xKWS9LYnio1MU8oi5iuTc'\n",
        "    downloaded9 = drive.CreateFile({'id':id}) \n",
        "    downloaded9.GetContentFile('stocktwits_FB.csv')\n",
        "    df_st9 = pd.read_csv('stocktwits_FB.csv')\n",
        "    return df_st9\n",
        "  elif comp=='GOOG':\n",
        "    #GOOG: https://drive.google.com/file/d/1UMnPXfG_XkgLJAQ2VtLvMPBPZhN68Ezw/view?usp=sharing\n",
        "    id = '1UMnPXfG_XkgLJAQ2VtLvMPBPZhN68Ezw'\n",
        "    downloaded10 = drive.CreateFile({'id':id}) \n",
        "    downloaded10.GetContentFile('stocktwits_GOOG.csv')\n",
        "    df_st10 = pd.read_csv('stocktwits_GOOG.csv')\n",
        "    return df_st10\n",
        "  elif comp=='GOOGL':\n",
        "    #GOOGL: https://drive.google.com/file/d/1NbCpQX0sTgXsqcW7xMbkrDPlAOscIoTL/view?usp=sharing\n",
        "    id = '1NbCpQX0sTgXsqcW7xMbkrDPlAOscIoTL'\n",
        "    downloaded11 = drive.CreateFile({'id':id}) \n",
        "    downloaded11.GetContentFile('stocktwits_GOOGL.csv')\n",
        "    df_st11 = pd.read_csv('stocktwits_GOOGL.csv')\n",
        "    return df_st11\n",
        "  elif comp=='HD':\n",
        "    #HD: https://drive.google.com/file/d/10wbCzJMcjLWAqrlEtHlWieIrq7dyolG2/view?usp=sharing\n",
        "    id = '10wbCzJMcjLWAqrlEtHlWieIrq7dyolG2'\n",
        "    downloaded12 = drive.CreateFile({'id':id}) \n",
        "    downloaded12.GetContentFile('stocktwits_HD.csv')\n",
        "    df_st12 = pd.read_csv('stocktwits_HD.csv')\n",
        "    return df_st12\n",
        "  elif comp=='INTC':\n",
        "    # INTC: https://drive.google.com/file/d/1k1-NSl8qLTa2oDs1G8CFC4CJzkv9mugw/view?usp=sharing \n",
        "    id = '1k1-NSl8qLTa2oDs1G8CFC4CJzkv9mugw'\n",
        "    downloaded13 = drive.CreateFile({'id':id}) \n",
        "    downloaded13.GetContentFile('stocktwits_INTC.csv')\n",
        "    df_st13 = pd.read_csv('stocktwits_INTC.csv')\n",
        "    return df_st13\n",
        "  elif comp=='JNJ':\n",
        "    # JNJ: https://drive.google.com/file/d/1Qiwu9vbDYU527szR8waaFDoFCyY4i_bc/view?usp=sharing\n",
        "    id = '1Qiwu9vbDYU527szR8waaFDoFCyY4i_bc'\n",
        "    downloaded14 = drive.CreateFile({'id':id}) \n",
        "    downloaded14.GetContentFile('stocktwits_JNJ.csv')\n",
        "    df_st14 = pd.read_csv('stocktwits_JNJ.csv')\n",
        "    return df_st14\n",
        "  elif comp=='NFLX':\n",
        "    # NFLX: https://drive.google.com/file/d/1DdJ8MPdgt9bxF3ZagkWUp45N4RMQ8Al6/view?usp=sharing\n",
        "    id = '1DdJ8MPdgt9bxF3ZagkWUp45N4RMQ8Al6'\n",
        "    downloaded15 = drive.CreateFile({'id':id}) \n",
        "    downloaded15.GetContentFile('stocktwits_NFLX.csv')\n",
        "    df_st15 = pd.read_csv('stocktwits_NFLX.csv')\n",
        "    return df_st15\n",
        "  elif comp=='PG':\n",
        "    # PG: https://drive.google.com/file/d/1tlueLaJhlduNMgRHk8Omkog5nNI8I8Yk/view?usp=sharing\n",
        "    id = '1tlueLaJhlduNMgRHk8Omkog5nNI8I8Yk'\n",
        "    downloaded16 = drive.CreateFile({'id':id}) \n",
        "    downloaded16.GetContentFile('stocktwits_PG.csv')\n",
        "    df_st16 = pd.read_csv('stocktwits_PG.csv')\n",
        "    return df_st16\n",
        "  elif comp=='QQQ':\n",
        "    # QQQ: https://drive.google.com/file/d/1gUsl5L4VBgsL9oqBxaUdk6tA9r4VxDjo/view?usp=sharing\n",
        "    id = '1gUsl5L4VBgsL9oqBxaUdk6tA9r4VxDjo'\n",
        "    downloaded17 = drive.CreateFile({'id':id}) \n",
        "    downloaded17.GetContentFile('stocktwits_QQQ.csv')\n",
        "    df_st17 = pd.read_csv('stocktwits_QQQ.csv')\n",
        "    return df_st17\n",
        "  elif comp=='SPY':\n",
        "    # SPY: https://drive.google.com/file/d/10s-zYQPIqlkNsUahgzDRqJGw0VAkn-R1/view?usp=sharing\n",
        "    id = '10s-zYQPIqlkNsUahgzDRqJGw0VAkn-R1'\n",
        "    downloaded18 = drive.CreateFile({'id':id}) \n",
        "    downloaded18.GetContentFile('stocktwits_SPY.csv')\n",
        "    df_st18 = pd.read_csv('stocktwits_SPY.csv')\n",
        "    return df_st18\n",
        "  elif comp=='T':\n",
        "    # T: https://drive.google.com/file/d/1rk3PsikhgrA7MxV28tUbzj7EOn9K5Ixu/view?usp=sharing\n",
        "    id = '1rk3PsikhgrA7MxV28tUbzj7EOn9K5Ixu'\n",
        "    downloaded19 = drive.CreateFile({'id':id}) \n",
        "    downloaded19.GetContentFile('stocktwits_T.csv')\n",
        "    df_st19 = pd.read_csv('stocktwits_T.csv')\n",
        "    return df_st19\n",
        "  elif comp=='TSLA':\n",
        "    # TSLA: https://drive.google.com/file/d/1on57uk2gd_CLsnj1dcRfuB_KsYydzEp2/view?usp=sharing\n",
        "    id = '1on57uk2gd_CLsnj1dcRfuB_KsYydzEp2'\n",
        "    downloaded20 = drive.CreateFile({'id':id}) \n",
        "    downloaded20.GetContentFile('stocktwits_TSLA.csv')\n",
        "    df_st20 = pd.read_csv('stocktwits_TSLA.csv')\n",
        "    return df_st20\n",
        "  elif comp=='UNH':\n",
        "    # UNH: https://drive.google.com/file/d/1zguMHb3pL2tCT4TYV8XJcP-dWTcq28mY/view?usp=sharing\n",
        "    id = '1zguMHb3pL2tCT4TYV8XJcP-dWTcq28mY'\n",
        "    downloaded21 = drive.CreateFile({'id':id}) \n",
        "    downloaded21.GetContentFile('stocktwits_UNH.csv')\n",
        "    df_st21 = pd.read_csv('stocktwits_UNH.csv')\n",
        "    return df_st21\n",
        "  elif comp=='V':\n",
        "    # V: https://drive.google.com/file/d/1qLI1Rsyf2ebZu53I8QaFuOQSeTyZShLg/view?usp=sharing\n",
        "    id = '1qLI1Rsyf2ebZu53I8QaFuOQSeTyZShLg'\n",
        "    downloaded22 = drive.CreateFile({'id':id}) \n",
        "    downloaded22.GetContentFile('stocktwits_V.csv')\n",
        "    df_st22 = pd.read_csv('stocktwits_V.csv')\n",
        "    return df_st22\n",
        "  elif comp=='VIX':\n",
        "    # VIX: https://drive.google.com/file/d/1SoIue0nfsn_GGMOroFg3tEp8re-v7PnJ/view?usp=sharing\n",
        "    id = '1SoIue0nfsn_GGMOroFg3tEp8re-v7PnJ'\n",
        "    downloaded23 = drive.CreateFile({'id':id}) \n",
        "    downloaded23.GetContentFile('stocktwits_VIX.csv')\n",
        "    df_st23 = pd.read_csv('stocktwits_VIX.csv')\n",
        "    return df_st23\n",
        "  elif comp=='VZ':\n",
        "    # VZ: https://drive.google.com/file/d/1ddISbB0qfDpM69senqEmmf6xbNWOpNUJ/view?usp=sharing\n",
        "    id = '1ddISbB0qfDpM69senqEmmf6xbNWOpNUJ'\n",
        "    downloaded24 = drive.CreateFile({'id':id}) \n",
        "    downloaded24.GetContentFile('stocktwits_VZ.csv')\n",
        "    df_st24 = pd.read_csv('stocktwits_VZ.csv')\n",
        "    return df_st24\n",
        "  elif comp=='WMT':\n",
        "    # WMT: https://drive.google.com/file/d/14Zdh1ZCj5RxZltXknG3Qisxhwzge5rkV/view?usp=sharing\n",
        "    id = '14Zdh1ZCj5RxZltXknG3Qisxhwzge5rkV'\n",
        "    downloaded25 = drive.CreateFile({'id':id}) \n",
        "    downloaded25.GetContentFile('stocktwits_WMT.csv')\n",
        "    df_st25 = pd.read_csv('stocktwits_WMT.csv')\n",
        "    return df_st25"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Change the company name here to what comapny we want to use data from (used the def from over)\n",
        "df_st1 = get_compData('HD')\n",
        "\n",
        "#Change the directory you want to save the file. This is used later, and we choose a location in our drive. \n",
        "PATH='/content/drive/My Drive/StockTwits Data Binary/'\n",
        "\n",
        "#Change the type of datetime column to datetime64[ns]\n",
        "df_st1['datetime'] = df_st1['datetime'].astype('datetime64[ns]')\n",
        "\n",
        "#Creates a new column called date, that for every datetime takes out the date\n",
        "df_st1['Date'] = [d.date() for d in df_st1['datetime']]\n",
        "\n",
        "#Creates a new column called time, that for every datetime takes out the time\n",
        "df_st1['Time'] = [d.time() for d in df_st1['datetime']]\n",
        "\n",
        "#Takes the last row of our dataframe and string from Date column. 0 is the first date, and -1 is the last one (indexing) \n",
        "start_dt = str(df_st1['Date'].iloc[-1])\n",
        "\n",
        "#Change the start_dt to the correct format; %Y-%m-%d, and uses timedelta to find our the diffence form the last date and add one day, so we will get data from all days and not miss the first one. \n",
        "start_dt = str((datetime.strptime(start_dt, \"%Y-%m-%d\") + timedelta(days=-1)).strftime(\"%Y-%m-%d\"))\n",
        "\n",
        "#Takes the first row of our dataframe and string from Date column. 0 is the first date, and -1 is the last one (indexing) \n",
        "end_dt = str(df_st1['Date'].iloc[0])\n",
        "\n",
        "#Change the end_dt to the correct format; %Y-%m-%d, and uses timedelta to find our the diffence form the first date and add one day, so we will get data from all days and not miss the first one. \n",
        "end_dt =  str((datetime.strptime(end_dt, \"%Y-%m-%d\") + timedelta(days=1)).strftime(\"%Y-%m-%d\"))\n",
        "\n",
        "#Sets the company to be what the string in the first row of symbol column.\n",
        "compny = df_st1['symbol'].iloc[0]\n",
        "\n",
        "#Sets the two path where we will save the files after labeling them. \n",
        "PATH2 = PATH+compny+'_binary_label2.2.csv'\n",
        "PATH = PATH+compny+'_binary_label2.1.csv'\n",
        "\n",
        "#Since Yahoo Fianace uses a diffrent symbol than StockTwits, e.g. a \"-\" and not \".\", we change them with this if, elif, elif, else code.\n",
        "if compny =='BRK.A':\n",
        "  compny = 'BRK-A'\n",
        "elif compny =='BRK.B':\n",
        "  compny ='BRK-B'\n",
        "elif compny =='VIX':\n",
        "  compny = '^VIX'\n",
        "else:\n",
        "  pass\n",
        "\n",
        "#Sets yahoo_data to be the downloaded data for the company, where we download data in the timespand between start_dt and end_dt.\n",
        "yahoo_data = yf.download(compny, start=start_dt, end=end_dt)\n",
        "\n",
        "#Resets the index to be level 0 in the datasets, so date wont be our index and we will have a number for each row instead.\n",
        "yahoo_data.reset_index(level=0, inplace=True)\n",
        "\n",
        "#Shows us the diffrent columns in the data we just downloaded\n",
        "yahoo_data.columns\n",
        "\n",
        "#set the start_dt and end_dt to new names.\n",
        "start_date = start_dt\n",
        "end_date = end_dt\n",
        "\n",
        "#we change the name of the yahoo_data to df_prices\n",
        "df_prices = pd.DataFrame(yahoo_data)\n",
        "\n",
        "#We change the format of the Date column for the dataset with Yahoo Fianance data.\n",
        "df_prices['Date'] =  pd.to_datetime(df_prices['Date'], format='%Y-%m-%d')\n",
        "\n",
        "#Tweet_dates adds all the dates for the company and stocktwits datafile to a list.\n",
        "tweet_dates = (df_st1['Date'].astype(str)).tolist()\n",
        "\n",
        "#Stock_dates adds all the dates from yahoo fiance dataframe to a list\n",
        "stock_dates = (df_prices['Date'].astype(str)).tolist()"
      ],
      "metadata": {
        "id": "2G4Yv4JR2Wj4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2a2017f-aed5-4495-9c1d-c28c04d39d2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_prices = pd.DataFrame(yahoo_data)\n",
        "all_days = pd.date_range(start=start_date, end=end_date, freq='D')\n",
        "\n",
        "df_prices['Date'] =  pd.to_datetime(df_prices['Date'], format='%Y/%m/%d')\n",
        "df_prices = df_prices.sort_values(by=['Date'], ascending=[True])\n",
        "df_prices.set_index('Date', inplace=True)\n",
        "df_prices = df_prices.reindex(all_days).reset_index().rename(columns={\"index\":\"Date\"})\n",
        "\n",
        "#Remove last row with NaN\n",
        "if np.isnan(df_prices.iloc[-1][1]):\n",
        "  df_prices.drop(df_prices.tail(1).index,inplace=True)\n",
        "\n",
        "#Fill missing values for missing dates by calculating mean of top and bottom row\n",
        "for i in range(len(df_prices)):\n",
        "  if np.isnan(df_prices.iloc[i][1]): # Continue if open price is NaN\n",
        "    counter = 1 \n",
        "    while counter != 0:\n",
        "      if ~np.isnan(df_prices.iloc[i+counter][1]): # Find the next non-NaN valued row using counter\n",
        "        for j in range(1, 7):\n",
        "          new = (df_prices.iloc[i - 1][j] + df_prices.iloc[i + counter][j]) / 2 # Calculate mean\n",
        "          df_prices.at[i, df_prices.columns[j]] = new\n",
        "        break\n",
        "      counter += 1\n",
        "\n",
        "#Calculate Price Change using Close price and Open Price of Same day\n",
        "price_change = []\n",
        "for i in range(len(df_prices)):\n",
        "  if df_prices.iloc[i][4] > df_prices.iloc[i][1]:\n",
        "    price_change.append(1)\n",
        "  else:\n",
        "    price_change.append(0)\n",
        "df_prices['PriceChange'] = price_change\n",
        "\n",
        "#Calculate Price Change using Close price of Previous day with Close price of today\n",
        "# price_change = []\n",
        "# for i in range(len(df_prices)):\n",
        "#   if df_prices.iloc[i-1][4] < df_prices.iloc[i][4]:\n",
        "#     price_change.append(1)\n",
        "#   else:\n",
        "#     price_change.append(0)\n",
        "# df_prices['PriceChange'] = price_change\n",
        "\n",
        "#Function to return Price Change value for a particular date\n",
        "def get_polarity (date):\n",
        "  rslt_df = df_prices[df_prices['Date'] == date] \n",
        "  return rslt_df['PriceChange'].values[0]\n",
        "\n",
        "#Assign polarity/label for each row using date\n",
        "for i in range(len(df_st1)):\n",
        "  # print(i)\n",
        "  polarity = get_polarity(df_st1.iloc[i][5].strftime(\"%Y-%m-%d\"))\n",
        "  df_st1.at[i, 'polarity'] = polarity\n",
        "\n",
        "#Check null or NaN values\n",
        "print(df_prices.isna().values.any()) #False\n",
        "print(df_prices.notnull().values.all()) #True\n",
        "print(df_prices.isnull().values.any()) #False\n",
        "\n",
        "#Convert float values of polarity/label to integer\n",
        "df_st1[\"polarity\"] = df_st1[\"polarity\"].astype(float).astype(int)\n",
        "\n",
        "#Rename polarity column to \"label\"\n",
        "df_st1.rename(columns = {'polarity':'label'}, inplace = True) \n",
        "\n",
        "#Drops values in labels colum that have the value of -99\n",
        "df_st1.drop(df_st1[df_st1['label'] == -99].index, inplace = True) "
      ],
      "metadata": {
        "id": "9R0dYvpH516L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00ee2425-9e25-47de-9eb8-b09ffd8c1ae0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n",
            "True\n",
            "False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Turns all messages to lowercase\n",
        "df_st1['message'] = df_st1['message'].str.lower()\n",
        "\n",
        "#Message is now all the messages fro stocktwits to a list\n",
        "message = df_st1['message'].tolist()\n",
        "\n",
        "#Definition for removing stopwords\n",
        "def remove_stopwords(msg):\n",
        "    filtered_sentence = [w for w in msg_tokens if not w in stop_words]\n",
        "    return filtered_sentence\n",
        "\n",
        "def remove_punctuation_re(x):\n",
        "    #Removing URLs\n",
        "    x = ' '.join(re.sub(\"https?://\\S+\",\"\",x).split())     \n",
        "\n",
        "    #Removing Mentions\n",
        "    x = ' '.join(re.sub(\"^@\\S+|\\s@\\S+\",\"\",x).split())     \n",
        "\n",
        "    x = ' '.join(re.sub(r'[^$\\w\\s]',\" \",x).split())\n",
        "    \n",
        "    #Removes Hashtags\n",
        "    x = ' '.join(re.sub(r'[^\\w\\s]',\" \",x).split())        \n",
        "\n",
        "    #Removing _ from emojis text\n",
        "    x = ' '.join(re.sub(r'_',\" \",x).split())              \n",
        "\n",
        "    return x\n",
        "\n",
        "def rpt_replace(match):\n",
        "    #print(match.group(1))\n",
        "    return match.group(1)+match.group(1)\n",
        "    \n",
        "def processRepeatings(data):\n",
        "    re_t= re.sub(message_rpt, rpt_replace, data )\n",
        "    #print(re_t)\n",
        "    return re_t\n",
        "\n",
        "#Download the stopword that is written of english\n",
        "stop_words = sw.words(\"english\")\n",
        "\n",
        "#Tweet_tokanizer, is a tokanizer that dont splits the hashtags from the word. \n",
        "tweet_tokenizer = TweetTokenizer()\n",
        "\n",
        "#Detokanizer \n",
        "detokenizer = TreebankWordDetokenizer()\n",
        "\n",
        "message_rpt = re.compile(r\"(.)\\1{2,}\", re.IGNORECASE)\n",
        "\n",
        "#Segmenter that will make hashtags into words. e.g., #netflixwillgoup = netflix will go up\n",
        "seg_tw = Segmenter(corpus=\"twitter\")\n",
        "\n",
        "message_p = []\n",
        "for msg in message:\n",
        "\n",
        "    if msg == '0': #nan replaced by '0'\n",
        "      message_p.append('-1')\n",
        "\n",
        "    else:\n",
        "      # remove emojis\n",
        "      msg = emoji.demojize(msg)\n",
        "\n",
        "      # fix contractions\n",
        "      msg = contractions.fix(msg)\n",
        "\n",
        "      # remove punctuations\n",
        "      msg = remove_punctuation_re(msg) \n",
        "\n",
        "      #tokenize\n",
        "      msg_tokens = tweet_tokenizer.tokenize(msg)\n",
        "\n",
        "      #For Hashtags elongated words using Word segmenter\n",
        "      message_seg = []\n",
        "      for w in msg_tokens:\n",
        "        if len(w)>=300:\n",
        "          w=w[:100]\n",
        "          print(w)\n",
        "        message_seg.append(seg_tw.segment(w))\n",
        "\n",
        "      # remove stopwords\n",
        "      msg = remove_stopwords(message_seg)\n",
        "\n",
        "      if 'rt' in msg:\n",
        "        # remove retweets\n",
        "        message_p.append('-1')\n",
        "      else: \n",
        "        # detokenize\n",
        "        msg = detokenizer.detokenize(msg)\n",
        "\n",
        "        # removing repeating characters like hurrrryyyyyy-- worrks on tokenized list\n",
        "        msg = processRepeatings(msg)\n",
        "\n",
        "        message_p.append(msg)\n",
        "\n",
        "df_st1['message'] = message_p\n",
        "\n",
        "df_st1.drop(df_st1[df_st1['message'] == '-1'].index, inplace = True) \n",
        "\n",
        "df_st1.to_csv(PATH2, header=True, index=False, encoding='utf_8')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gnBE6d_E57Ck",
        "outputId": "ff961060-02d4-433d-decc-9efe11bb0319"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading twitter - 1grams ...\n",
            "Reading twitter - 2grams ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "t6nA__ueIJ9p"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}