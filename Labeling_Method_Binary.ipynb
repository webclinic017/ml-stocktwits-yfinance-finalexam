{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Labeling Method Binary.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hMJ4bwWr2Nfg"
      },
      "outputs": [],
      "source": [
        "!pip install contractions\n",
        "!pip install emoji\n",
        "!pip install ekphrasis\n",
        "!pip install -U -q PyDrive\n",
        "!pip install yfinance --upgrade --no-cache-dir\n",
        "\n",
        "import contractions\n",
        "import datetime\n",
        "from datetime import datetime , timedelta, date\n",
        "import emoji\n",
        "import itertools\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.ticker import FormatStrFormatter, StrMethodFormatter\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords as sw\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import TweetTokenizer, word_tokenize\n",
        "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import pprint\n",
        "from pprint import pprint\n",
        "import random\n",
        "import re\n",
        "import requests \n",
        "import seaborn as sns\n",
        "import string\n",
        "import sys\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from tensorflow import keras\n",
        "import torch\n",
        "import yfinance as yf\n",
        "from ekphrasis.classes.segmenter import Segmenter\n",
        "from pydrive.auth  import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth,  drive\n",
        "from oauth2client.client import GoogleCredentials\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "def get_compData(comp):\n",
        "  if comp =='AAPL':\n",
        "    id = '1diLooyj8DtyyxwiihpbTJdfY4D98irLE'\n",
        "    downloaded1 = drive.CreateFile({'id':id}) \n",
        "    downloaded1.GetContentFile('stocktwits_AAPL.csv')\n",
        "    df_st1 = pd.read_csv('stocktwits_AAPL.csv')\n",
        "    return df_st1\n",
        "  elif comp=='ADBE':\n",
        "    id = '1SMyRg8aUnnQTbukVadBo814qdS6xDnP1'\n",
        "    downloaded2 = drive.CreateFile({'id':id}) \n",
        "    downloaded2.GetContentFile('stocktwits_ADBE.csv')\n",
        "    df_st2 = pd.read_csv('stocktwits_ADBE.csv')\n",
        "    return df_st2\n",
        "  elif comp=='AMZN':\n",
        "    id = '1ffmFOrlcaTCCByKMq7E1LCXXK6daMkB_'\n",
        "    downloaded3 = drive.CreateFile({'id':id}) \n",
        "    downloaded3.GetContentFile('stocktwits_AMZN.csv')\n",
        "    df_st3 = pd.read_csv('stocktwits_AMZN.csv')\n",
        "    return df_st3\n",
        "  elif comp=='BAC':\n",
        "    id = '1HeaKJtlSz2xiLT3sx5FLdYmzCkVDGIjW'\n",
        "    downloaded4 = drive.CreateFile({'id':id}) \n",
        "    downloaded4.GetContentFile('stocktwits_BAC.csv')\n",
        "    df_st4 = pd.read_csv('stocktwits_BAC.csv')\n",
        "    return df_st4\n",
        "  elif comp=='BRK.A':\n",
        "    id = '1HeQhAU20YyT1tRCD-yN83vrUGY7jDDSt'\n",
        "    downloaded5 = drive.CreateFile({'id':id}) \n",
        "    downloaded5.GetContentFile('stocktwits_BRK_A.csv')\n",
        "    df_st5 = pd.read_csv('stocktwits_BRK_A.csv')\n",
        "    return df_st5\n",
        "  elif comp=='BRK.B':\n",
        "    id = '1FRrwrIVJVyFqg025SWasKW84qOJMhe84'\n",
        "    downloaded6 = drive.CreateFile({'id':id}) \n",
        "    downloaded6.GetContentFile('stocktwits_BRK_B.csv')\n",
        "    df_st6 = pd.read_csv('stocktwits_BRK_B.csv')\n",
        "    return df_st6\n",
        "  elif comp=='DIA':\n",
        "    id = '1riZ8IdkupLre9NQ7McBO21wDFVX_NPvg'\n",
        "    downloaded7 = drive.CreateFile({'id':id}) \n",
        "    downloaded7.GetContentFile('stocktwits_DIA.csv')\n",
        "    df_st7 = pd.read_csv('stocktwits_DIA.csv')\n",
        "    return df_st7\n",
        "  elif comp=='DIS':\n",
        "    id = '1y5IT3gA_yIJnFLTsxy1GHdIY4SHwj84Y'\n",
        "    downloaded8 = drive.CreateFile({'id':id}) \n",
        "    downloaded8.GetContentFile('stocktwits_DIS.csv')\n",
        "    df_st8 = pd.read_csv('stocktwits_DIS.csv')\n",
        "    return df_st8\n",
        "  elif comp=='FB':\n",
        "    id = '1x1FugJhUQx9xKWS9LYnio1MU8oi5iuTc'\n",
        "    downloaded9 = drive.CreateFile({'id':id}) \n",
        "    downloaded9.GetContentFile('stocktwits_FB.csv')\n",
        "    df_st9 = pd.read_csv('stocktwits_FB.csv')\n",
        "    return df_st9\n",
        "  elif comp=='GOOG':\n",
        "    id = '1UMnPXfG_XkgLJAQ2VtLvMPBPZhN68Ezw'\n",
        "    downloaded10 = drive.CreateFile({'id':id}) \n",
        "    downloaded10.GetContentFile('stocktwits_GOOG.csv')\n",
        "    df_st10 = pd.read_csv('stocktwits_GOOG.csv')\n",
        "    return df_st10\n",
        "  elif comp=='GOOGL':\n",
        "    id = '1NbCpQX0sTgXsqcW7xMbkrDPlAOscIoTL'\n",
        "    downloaded11 = drive.CreateFile({'id':id}) \n",
        "    downloaded11.GetContentFile('stocktwits_GOOGL.csv')\n",
        "    df_st11 = pd.read_csv('stocktwits_GOOGL.csv')\n",
        "    return df_st11\n",
        "  elif comp=='HD':\n",
        "    id = '10wbCzJMcjLWAqrlEtHlWieIrq7dyolG2'\n",
        "    downloaded12 = drive.CreateFile({'id':id}) \n",
        "    downloaded12.GetContentFile('stocktwits_HD.csv')\n",
        "    df_st12 = pd.read_csv('stocktwits_HD.csv')\n",
        "    return df_st12\n",
        "  elif comp=='INTC':\n",
        "    id = '1k1-NSl8qLTa2oDs1G8CFC4CJzkv9mugw'\n",
        "    downloaded13 = drive.CreateFile({'id':id}) \n",
        "    downloaded13.GetContentFile('stocktwits_INTC.csv')\n",
        "    df_st13 = pd.read_csv('stocktwits_INTC.csv')\n",
        "    return df_st13\n",
        "  elif comp=='JNJ':\n",
        "    id = '1Qiwu9vbDYU527szR8waaFDoFCyY4i_bc'\n",
        "    downloaded14 = drive.CreateFile({'id':id}) \n",
        "    downloaded14.GetContentFile('stocktwits_JNJ.csv')\n",
        "    df_st14 = pd.read_csv('stocktwits_JNJ.csv')\n",
        "    return df_st14\n",
        "  elif comp=='NFLX':\n",
        "    id = '1DdJ8MPdgt9bxF3ZagkWUp45N4RMQ8Al6'\n",
        "    downloaded15 = drive.CreateFile({'id':id}) \n",
        "    downloaded15.GetContentFile('stocktwits_NFLX.csv')\n",
        "    df_st15 = pd.read_csv('stocktwits_NFLX.csv')\n",
        "    return df_st15\n",
        "  elif comp=='PG':\n",
        "    id = '1tlueLaJhlduNMgRHk8Omkog5nNI8I8Yk'\n",
        "    downloaded16 = drive.CreateFile({'id':id}) \n",
        "    downloaded16.GetContentFile('stocktwits_PG.csv')\n",
        "    df_st16 = pd.read_csv('stocktwits_PG.csv')\n",
        "    return df_st16\n",
        "  elif comp=='QQQ':\n",
        "    id = '1gUsl5L4VBgsL9oqBxaUdk6tA9r4VxDjo'\n",
        "    downloaded17 = drive.CreateFile({'id':id}) \n",
        "    downloaded17.GetContentFile('stocktwits_QQQ.csv')\n",
        "    df_st17 = pd.read_csv('stocktwits_QQQ.csv')\n",
        "    return df_st17\n",
        "  elif comp=='SPY':\n",
        "    id = '10s-zYQPIqlkNsUahgzDRqJGw0VAkn-R1'\n",
        "    downloaded18 = drive.CreateFile({'id':id}) \n",
        "    downloaded18.GetContentFile('stocktwits_SPY.csv')\n",
        "    df_st18 = pd.read_csv('stocktwits_SPY.csv')\n",
        "    return df_st18\n",
        "  elif comp=='T':\n",
        "    id = '1rk3PsikhgrA7MxV28tUbzj7EOn9K5Ixu'\n",
        "    downloaded19 = drive.CreateFile({'id':id}) \n",
        "    downloaded19.GetContentFile('stocktwits_T.csv')\n",
        "    df_st19 = pd.read_csv('stocktwits_T.csv')\n",
        "    return df_st19\n",
        "  elif comp=='TSLA':\n",
        "    id = '1on57uk2gd_CLsnj1dcRfuB_KsYydzEp2'\n",
        "    downloaded20 = drive.CreateFile({'id':id}) \n",
        "    downloaded20.GetContentFile('stocktwits_TSLA.csv')\n",
        "    df_st20 = pd.read_csv('stocktwits_TSLA.csv')\n",
        "    return df_st20\n",
        "  elif comp=='UNH':\n",
        "    id = '1zguMHb3pL2tCT4TYV8XJcP-dWTcq28mY'\n",
        "    downloaded21 = drive.CreateFile({'id':id}) \n",
        "    downloaded21.GetContentFile('stocktwits_UNH.csv')\n",
        "    df_st21 = pd.read_csv('stocktwits_UNH.csv')\n",
        "    return df_st21\n",
        "  elif comp=='V':\n",
        "    id = '1qLI1Rsyf2ebZu53I8QaFuOQSeTyZShLg'\n",
        "    downloaded22 = drive.CreateFile({'id':id}) \n",
        "    downloaded22.GetContentFile('stocktwits_V.csv')\n",
        "    df_st22 = pd.read_csv('stocktwits_V.csv')\n",
        "    return df_st22\n",
        "  elif comp=='VIX':\n",
        "    id = '1SoIue0nfsn_GGMOroFg3tEp8re-v7PnJ'\n",
        "    downloaded23 = drive.CreateFile({'id':id}) \n",
        "    downloaded23.GetContentFile('stocktwits_VIX.csv')\n",
        "    df_st23 = pd.read_csv('stocktwits_VIX.csv')\n",
        "    return df_st23\n",
        "  elif comp=='VZ':\n",
        "    id = '1ddISbB0qfDpM69senqEmmf6xbNWOpNUJ'\n",
        "    downloaded24 = drive.CreateFile({'id':id}) \n",
        "    downloaded24.GetContentFile('stocktwits_VZ.csv')\n",
        "    df_st24 = pd.read_csv('stocktwits_VZ.csv')\n",
        "    return df_st24\n",
        "  elif comp=='WMT':\n",
        "    id = '14Zdh1ZCj5RxZltXknG3Qisxhwzge5rkV'\n",
        "    downloaded25 = drive.CreateFile({'id':id}) \n",
        "    downloaded25.GetContentFile('stocktwits_WMT.csv')\n",
        "    df_st25 = pd.read_csv('stocktwits_WMT.csv')\n",
        "    return df_st25"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_st1 = get_compData('COMPANY')#Replace with comapny stock ticker\n",
        "PATH='/content/drive/My Drive/StockTwits Data Binary/'\n",
        "df_st1['datetime'] = df_st1['datetime'].astype('datetime64[ns]')\n",
        "df_st1['Date'] = [d.date() for d in df_st1['datetime']]\n",
        "\n",
        "#For AAPL, ADBE, BRK.A, BRK.B, GOOG, GOOGL\n",
        "# df_st1 = df_st1.drop(df_st1[df_st1['Date'] == df_st1['Date'][0]].index, inplace = False)\n",
        "# df_st1 = df_st1.reset_index(drop=True)\n",
        "# df_st1 = df_st1.drop(df_st1[df_st1['Date'] == df_st1['Date'][0]].index, inplace = False)\n",
        "# df_st1 = df_st1.reset_index(drop=True)\n",
        "\n",
        "df_st1['Time'] = [d.time() for d in df_st1['datetime']]\n",
        "start_dt = str(df_st1['Date'].iloc[-1])\n",
        "start_dt = str((datetime.strptime(start_dt, \"%Y-%m-%d\") + timedelta(days=-1)).strftime(\"%Y-%m-%d\"))\n",
        "end_dt = str(df_st1['Date'].iloc[0])\n",
        "end_dt =  str((datetime.strptime(end_dt, \"%Y-%m-%d\") + timedelta(days=1)).strftime(\"%Y-%m-%d\"))\n",
        "compny = df_st1['symbol'].iloc[0]\n",
        "PATH2 = PATH+compny+'_binary_label2.2.csv'#PREVDAY\n",
        "PATH = PATH+compny+'_binary_label2.1.csv'#SAMEDAY\n",
        "if compny =='BRK.A':\n",
        "  compny = 'BRK-A'\n",
        "elif compny =='BRK.B':\n",
        "  compny ='BRK-B'\n",
        "elif compny =='VIX':\n",
        "  compny = '^VIX'\n",
        "else:\n",
        "  pass\n",
        "yahoo_data = yf.download(compny, start=start_dt, end=end_dt)\n",
        "yahoo_data.reset_index(level=0, inplace=True)\n",
        "yahoo_data.columns\n",
        "start_date = start_dt\n",
        "end_date = end_dt\n",
        "df_prices = pd.DataFrame(yahoo_data)\n",
        "df_prices['Date'] =  pd.to_datetime(df_prices['Date'], format='%Y-%m-%d')\n",
        "tweet_dates = (df_st1['Date'].astype(str)).tolist()\n",
        "stock_dates = (df_prices['Date'].astype(str)).tolist()\n",
        "df_prices = pd.DataFrame(yahoo_data)\n",
        "all_days = pd.date_range(start=start_date, end=end_date, freq='D')\n",
        "df_prices['Date'] =  pd.to_datetime(df_prices['Date'], format='%Y/%m/%d')\n",
        "df_prices = df_prices.sort_values(by=['Date'], ascending=[True])\n",
        "df_prices.set_index('Date', inplace=True)\n",
        "df_prices = df_prices.reindex(all_days).reset_index().rename(columns={\"index\":\"Date\"})\n",
        "if np.isnan(df_prices.iloc[-1][1]):\n",
        "  df_prices.drop(df_prices.tail(1).index,inplace=True)\n",
        "for i in range(len(df_prices)):\n",
        "  if np.isnan(df_prices.iloc[i][1]):\n",
        "    counter = 1 \n",
        "    while counter != 0:\n",
        "      if ~np.isnan(df_prices.iloc[i+counter][1]):\n",
        "        for j in range(1, 7):\n",
        "          new = (df_prices.iloc[i - 1][j] + df_prices.iloc[i + counter][j]) / 2\n",
        "          df_prices.at[i, df_prices.columns[j]] = new\n",
        "        break\n",
        "      counter += 1\n",
        "\n",
        "# price_change = []\n",
        "# for i in range(len(df_prices)):\n",
        "#   if df_prices.iloc[i][4] > df_prices.iloc[i][1]:\n",
        "#     price_change.append(1)\n",
        "#   else:\n",
        "#     price_change.append(0)\n",
        "# df_prices['PriceChange'] = price_change\n",
        "\n",
        "price_change = []\n",
        "for i in range(len(df_prices)):\n",
        "  if df_prices.iloc[i-1][4] < df_prices.iloc[i][4]:\n",
        "    price_change.append(1)\n",
        "  else:\n",
        "    price_change.append(0)\n",
        "df_prices['PriceChange'] = price_change\n",
        "\n",
        "def get_polarity (date):\n",
        "  rslt_df = df_prices[df_prices['Date'] == date] \n",
        "  return rslt_df['PriceChange'].values[0]\n",
        "for i in range(len(df_st1)):\n",
        "  polarity = get_polarity(df_st1.iloc[i][5].strftime(\"%Y-%m-%d\"))\n",
        "  df_st1.at[i, 'polarity'] = polarity\n",
        "df_st1[\"polarity\"] = df_st1[\"polarity\"].astype(float).astype(int)\n",
        "df_st1.rename(columns = {'polarity':'label'}, inplace = True) \n",
        "df_st1.drop(df_st1[df_st1['label'] == -99].index, inplace = True) \n",
        "df_st1['message'] = df_st1['message'].str.lower()\n",
        "message = df_st1['message'].tolist()\n",
        "def remove_stopwords(msg):\n",
        "    filtered_sentence = [w for w in msg_tokens if not w in stop_words]\n",
        "    return filtered_sentence\n",
        "def remove_punctuation_re(x):\n",
        "    x = ' '.join(re.sub(\"https?://\\S+\",\"\",x).split())     \n",
        "    x = ' '.join(re.sub(\"^@\\S+|\\s@\\S+\",\"\",x).split())     \n",
        "    x = ' '.join(re.sub(r'[^$\\w\\s]',\" \",x).split())\n",
        "    x = ' '.join(re.sub(r'[^\\w\\s]',\" \",x).split())        \n",
        "    x = ' '.join(re.sub(r'_',\" \",x).split())              \n",
        "    return x\n",
        "def rpt_replace(match):\n",
        "    return match.group(1)+match.group(1)   \n",
        "def processRepeatings(data):\n",
        "    re_t= re.sub(message_rpt, rpt_replace, data )\n",
        "    return re_t\n",
        "stop_words = sw.words(\"english\")\n",
        "tweet_tokenizer = TweetTokenizer()\n",
        "detokenizer = TreebankWordDetokenizer()\n",
        "message_rpt = re.compile(r\"(.)\\1{2,}\", re.IGNORECASE)\n",
        "seg_tw = Segmenter(corpus=\"twitter\")\n",
        "message_p = []\n",
        "for msg in message:\n",
        "    if msg == '0':\n",
        "      message_p.append('-1')\n",
        "    else:\n",
        "      msg = emoji.demojize(msg)\n",
        "      msg = contractions.fix(msg)\n",
        "      msg = remove_punctuation_re(msg) \n",
        "      msg_tokens = tweet_tokenizer.tokenize(msg)\n",
        "      message_seg = []\n",
        "      for w in msg_tokens:\n",
        "        if len(w)>=300:\n",
        "          w=w[:100]\n",
        "          print(w)\n",
        "        message_seg.append(seg_tw.segment(w))\n",
        "      msg = remove_stopwords(message_seg)\n",
        "      if 'rt' in msg:\n",
        "        message_p.append('-1')\n",
        "      else: \n",
        "        msg = detokenizer.detokenize(msg)\n",
        "        msg = processRepeatings(msg)\n",
        "        message_p.append(msg)\n",
        "df_st1['message'] = message_p\n",
        "df_st1.drop(df_st1[df_st1['message'] == '-1'].index, inplace = True) \n",
        "df_st1.to_csv(PATH2, header=True, index=False, encoding='utf_8')"
      ],
      "metadata": {
        "id": "YD-h4Sg_C5rr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}